{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assessment we'll be using the short story [_An Occurrence at Owl Creek Bridge_](https://en.wikipedia.org/wiki/An_Occurrence_at_Owl_Creek_Bridge) by Ambrose Bierce (1890). <br>The story is in the public domain; the text file was obtained from [Project Gutenberg](https://www.gutenberg.org/ebooks/375.txt.utf-8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create a Doc object from the file `owlcreek.txt`**<br>\n",
    "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = r'C:\\Users\\iamke\\OneDrive\\Desktop\\NLP\\Owlcreek.txt'\n",
    "\n",
    "# Try to open the file and print its contents\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        # print(text)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not find file at path: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "An Occurrence at Owl Creek Bridge\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "THE MILLENNIUM FULCRUM EDITION, 1988\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "I\n",
       "\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(doc[:36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How many tokens are contained in the file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4682\n"
     ]
    }
   ],
   "source": [
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How many sentences are contained in the file?**<br>HINT: You'll want to build a list first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Print the second sentence in the document**<br> HINT: Indexing starts at zero, and the title counts as the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "\n",
      "\n",
      "A man stood upon a railroad bridge in northern Alabama, looking down\n",
      "into the swift water twenty feet below.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. For each token in the sentence above, print its `text`, `POS` tag, `dep` tag and `lemma`<br>\n",
    "CHALLENGE: Have values line up in columns in the print output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET det the\n",
      "man NOUN poss man\n",
      "’s PART case ’s\n",
      "hands NOUN nsubj hand\n",
      "were AUX ROOT be\n",
      "behind ADP prep behind\n",
      "his PRON poss his\n",
      "\n",
      " SPACE dep \n",
      "\n",
      "back NOUN pobj back\n",
      ", PUNCT punct ,\n",
      "the DET det the\n",
      "wrists NOUN npadvmod wrist\n",
      "bound VERB acl bind\n",
      "with ADP prep with\n",
      "a DET det a\n",
      "cord NOUN pobj cord\n",
      ". PUNCT punct .\n"
     ]
    }
   ],
   "source": [
    " if len(sentences) > 1:\n",
    "    \n",
    "        second_sentence = sentences[2]\n",
    "        doc = nlp(second_sentence.text)\n",
    "        \n",
    "        for token in doc:\n",
    "            print(f\"{token.text} {token.pos_} {token.dep_} {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The             DET        det        the            \n",
      "man             NOUN       poss       man            \n",
      "’s              PART       case       ’s             \n",
      "hands           NOUN       nsubj      hand           \n",
      "were            AUX        ROOT       be             \n",
      "behind          ADP        prep       behind         \n",
      "his             PRON       poss       his            \n",
      "\n",
      "               SPACE      dep        \n",
      "              \n",
      "back            NOUN       pobj       back           \n",
      ",               PUNCT      punct      ,              \n",
      "the             DET        det        the            \n",
      "wrists          NOUN       appos      wrist          \n",
      "bound           VERB       acl        bind           \n",
      "with            ADP        prep       with           \n",
      "a               DET        det        a              \n",
      "cord            NOUN       pobj       cord           \n",
      ".               PUNCT      punct      .              \n"
     ]
    }
   ],
   "source": [
    "for token in second_sentence:\n",
    "    print(f\"{token.text:<15} {token.pos_:<10} {token.dep_:<10} {token.lemma_:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text**<br>\n",
    "HINT: You should include an `'IS_SPACE': True` pattern between the two words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher library:\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "       \n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "         {\"LOWER\": \"swimming\"},\n",
    "        {\"LOWER\": \"vigorously\"}\n",
    "    ]\n",
    "\n",
    "matcher.add(\"Swimming\", [pattern])\n",
    "matches = matcher(doc)\n",
    "found_matches = []\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "        matched_span = doc[start:end]\n",
    "        found_matches.append(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swimming\\nvigorously', 'swimming\\nvigorously']\n"
     ]
    }
   ],
   "source": [
    "found_matches = [doc[start:end].text for match_id, start, end in matches]\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Print the text surrounding each found match**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context around match 'swimming\n",
      "vigorously':\n",
      "my hands,” he thought, “I might throw off the noose and spring\n",
      "into the stream. By diving I could evade the bullets and, swimming\n",
      "vigorously, reach the bank, take to the woods and get away home. My\n",
      "home, thank God, is as yet outside their lines; my wife\n",
      "\n",
      "==================================================\n",
      "\n",
      "Context around match 'swimming\n",
      "vigorously':\n",
      "thrust into their sockets. The two\n",
      "sentinels fired again, independently and ineffectually.\n",
      "\n",
      "The hunted man saw all this over his shoulder; he was now swimming\n",
      "vigorously with the current. His brain was as energetic as his arms and\n",
      "legs; he thought with the rapidity of lightning:\n",
      "\n",
      "“The officer,”\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_size = 30\n",
    "for match_id, start, end in matches:\n",
    "    start_context = max(0, start - context_size)\n",
    "    end_context = min(len(doc), end + context_size)\n",
    "    context = doc[start_context:end_context]\n",
    "    print(f\"Context around match '{doc[start:end]}':\")\n",
    "    print(context.text)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context around match 'swimming\n",
      "vigorously':\n",
      " could evade the bullets and, [swimming\n",
      "vigorously], reach the bank, take to the \n",
      "\n",
      "==================================================\n",
      "\n",
      "Context around match 'swimming\n",
      "vigorously':\n",
      "over his shoulder; he was now [swimming\n",
      "vigorously] with the current. His brain w\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    start_char = max(0, doc[start].idx - context_size)\n",
    "    end_char = min(len(doc.text), doc[end - 1].idx + len(doc[end - 1].text) + context_size)\n",
    "    context = doc.text[start_char:end_char]\n",
    "    match_text = doc[start:end].text\n",
    "    print(f\"Context around match '{match_text}':\")\n",
    "    print(context.replace(match_text, f\"[{match_text}]\"))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
